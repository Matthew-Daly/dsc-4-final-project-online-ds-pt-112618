{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook was used to run a grid search for the optimal batch size and epochs for the final model. \n",
    "{'batch_size': 50, 'epochs': 15})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.style as style\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Conv2D, MaxPooling2D, Flatten, Dropout\n",
    "import matplotlib.style as style\n",
    "from keras import layers\n",
    "from keras import models\n",
    "from keras import optimizers\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from keras.optimizers import SGD\n",
    "from keras.constraints import maxnorm\n",
    "style.use('fivethirtyeight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv('sign-language-mnist/sign_mnist_train.csv')\n",
    "df_test = pd.read_csv('sign-language-mnist/sign_mnist_test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = df_train['label'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.drop('label', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "images = df_train.values\n",
    "images = np.array([np.reshape(i, (28, 28)) for i in images])\n",
    "images = np.array([i.flatten() for i in images])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_bin = LabelBinarizer()\n",
    "labels = label_bin.fit_transform(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(images, labels, test_size = 0.3, random_state = 101)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = x_train / 255\n",
    "x_test = x_test / 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = x_train.reshape(x_train.shape[0], 28, 28, 1)\n",
    "x_test = x_test.reshape(x_test.shape[0], 28, 28, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model():\n",
    "    \n",
    "    activation = 'relu'\n",
    "    dropout_rate = 0.0\n",
    "    init_mode = 'uniform'\n",
    "    weight_constraint=0\n",
    "    optimizer = 'adam'\n",
    "    lr = 0.01\n",
    "    momemntum=0\n",
    "    \n",
    "    \n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(64, kernel_size=(3,3), activation = 'relu', input_shape=(28, 28 ,1) ))\n",
    "    model.add(MaxPooling2D(pool_size = (2, 2)))\n",
    "\n",
    "    model.add(Conv2D(64, kernel_size = (3, 3), activation = 'relu'))\n",
    "    model.add(MaxPooling2D(pool_size = (2, 2)))\n",
    "\n",
    "    model.add(Conv2D(64, kernel_size = (3, 3), activation = 'relu'))\n",
    "    model.add(MaxPooling2D(pool_size = (2, 2)))\n",
    "\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(128, activation = 'relu'))\n",
    "    model.add(Dropout(0.20))\n",
    "    model.add(Dense(24, activation = 'softmax'))\n",
    "    \n",
    "    model.compile(loss = keras.losses.categorical_crossentropy, optimizer=keras.optimizers.Adam(),\n",
    "              metrics=['accuracy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = KerasClassifier(build_fn=create_model, batch_size=130, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "activation = ['relu', 'tanh', 'sigmoid', 'hard_sigmoid', 'linear', 'softmax']\n",
    "momentum= [0.0, 0.2, 0.4, 0.6, 0.8, 0.9]\n",
    "learn_rate = [0.001, 0.01, 0.2, 0.3]\n",
    "dropout_rate = [0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]\n",
    "weight_constraint = [1, 2, 3, 4, 5]\n",
    "neurons = [1, 5, 10, 15, 20, 25, 30]\n",
    "init = ['uniform', 'lecun_uniform', 'normal', 'zero', 'glorot_normal', 'glorot_uniform', 'he_normal', 'he_uniform']\n",
    "optimizers = ['SGD', 'RMSprop', 'Adagrad', 'Adadelta', 'Adam', 'Adamax', 'Nadam']\n",
    "epochs = [5, 10, 15, 20]\n",
    "batch_size= [50, 100, 150, 200]\n",
    "param_grid = dict(epochs=epochs, batch_size=batch_size)\n",
    "grid = GridSearchCV(estimator=model, param_grid=param_grid, n_jobs=-1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/matthew/anaconda3/lib/python3.7/site-packages/sklearn/model_selection/_split.py:2053: FutureWarning: You should specify a value for 'cv' instead of relying on the default value. The default value will change from 3 to 5 in version 0.22.\n",
      "  warnings.warn(CV_WARNING, FutureWarning)\n",
      "/home/matthew/anaconda3/lib/python3.7/site-packages/sklearn/externals/joblib/externals/loky/process_executor.py:706: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
      "  \"timeout or by a memory leak.\", UserWarning\n",
      "/home/matthew/anaconda3/lib/python3.7/site-packages/sklearn/externals/joblib/externals/loky/process_executor.py:706: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
      "  \"timeout or by a memory leak.\", UserWarning\n",
      "/home/matthew/anaconda3/lib/python3.7/site-packages/sklearn/externals/joblib/externals/loky/process_executor.py:706: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
      "  \"timeout or by a memory leak.\", UserWarning\n",
      "/home/matthew/anaconda3/lib/python3.7/site-packages/sklearn/externals/joblib/externals/loky/process_executor.py:706: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
      "  \"timeout or by a memory leak.\", UserWarning\n",
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0730 23:48:37.791685 140087546091264 deprecation_wrapper.py:119] From /home/matthew/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "W0730 23:48:39.098512 140087546091264 deprecation_wrapper.py:119] From /home/matthew/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "W0730 23:48:39.699238 140087546091264 deprecation_wrapper.py:119] From /home/matthew/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "W0730 23:48:40.823939 140087546091264 deprecation_wrapper.py:119] From /home/matthew/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:3976: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
      "\n",
      "W0730 23:48:41.242622 140087546091264 deprecation_wrapper.py:119] From /home/matthew/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:133: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
      "\n",
      "W0730 23:48:41.501076 140087546091264 deprecation.py:506] From /home/matthew/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "W0730 23:48:46.489168 140087546091264 deprecation_wrapper.py:119] From /home/matthew/anaconda3/lib/python3.7/site-packages/keras/optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "W0730 23:48:46.503461 140087546091264 deprecation_wrapper.py:119] From /home/matthew/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:3295: The name tf.log is deprecated. Please use tf.math.log instead.\n",
      "\n",
      "W0730 23:48:46.644257 140087546091264 deprecation.py:323] From /home/matthew/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "19218/19218 [==============================] - 18s 940us/step - loss: 2.0782 - acc: 0.3359\n",
      "Epoch 2/15\n",
      "19218/19218 [==============================] - 15s 769us/step - loss: 0.7037 - acc: 0.7569\n",
      "Epoch 3/15\n",
      "19218/19218 [==============================] - 13s 699us/step - loss: 0.3358 - acc: 0.8869\n",
      "Epoch 4/15\n",
      "19218/19218 [==============================] - 13s 692us/step - loss: 0.1789 - acc: 0.9417\n",
      "Epoch 5/15\n",
      "19218/19218 [==============================] - 13s 695us/step - loss: 0.1100 - acc: 0.9643\n",
      "Epoch 6/15\n",
      "19218/19218 [==============================] - 14s 712us/step - loss: 0.0666 - acc: 0.9801\n",
      "Epoch 7/15\n",
      "19218/19218 [==============================] - 17s 888us/step - loss: 0.0478 - acc: 0.9858\n",
      "Epoch 8/15\n",
      "19218/19218 [==============================] - 14s 739us/step - loss: 0.0333 - acc: 0.9896\n",
      "Epoch 9/15\n",
      "19218/19218 [==============================] - 14s 741us/step - loss: 0.0287 - acc: 0.9918\n",
      "Epoch 10/15\n",
      "19218/19218 [==============================] - 14s 739us/step - loss: 0.0260 - acc: 0.9927\n",
      "Epoch 11/15\n",
      "19218/19218 [==============================] - 15s 779us/step - loss: 0.0217 - acc: 0.9938\n",
      "Epoch 12/15\n",
      "19218/19218 [==============================] - 15s 756us/step - loss: 0.0183 - acc: 0.9947\n",
      "Epoch 13/15\n",
      "19218/19218 [==============================] - 15s 779us/step - loss: 0.0242 - acc: 0.9920\n",
      "Epoch 14/15\n",
      "19218/19218 [==============================] - 14s 744us/step - loss: 0.0146 - acc: 0.9959\n",
      "Epoch 15/15\n",
      "19218/19218 [==============================] - 14s 741us/step - loss: 0.0203 - acc: 0.9933\n"
     ]
    }
   ],
   "source": [
    "grid_result = grid.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.9983869303040728, {'batch_size': 50, 'epochs': 15})\n"
     ]
    }
   ],
   "source": [
    "print((grid_result.best_score_, grid_result.best_params_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Best batch_size = 50, best_epoch = 15"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
